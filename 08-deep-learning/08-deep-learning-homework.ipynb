{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4ef399-85c7-4e05-a3c5-57c76cb5dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f47ee4-8c83-4033-96aa-cdaf3bf149d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11de41d-7cf1-4ece-afae-c41055699552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class HairDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with train/test folders containing curly/straight subfolders\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load curly hair images (label = 1)\n",
    "        curly_dir = self.root_dir / 'curly'\n",
    "        if curly_dir.exists():\n",
    "            for img_path in curly_dir.glob('*.jpg'):\n",
    "                self.images.append(str(img_path))\n",
    "                self.labels.append(1)\n",
    "            for img_path in curly_dir.glob('*.png'):\n",
    "                self.images.append(str(img_path))\n",
    "                self.labels.append(1)\n",
    "        \n",
    "        # Load straight hair images (label = 0)\n",
    "        straight_dir = self.root_dir / 'straight'\n",
    "        if straight_dir.exists():\n",
    "            for img_path in straight_dir.glob('*.jpg'):\n",
    "                self.images.append(str(img_path))\n",
    "                self.labels.append(0)\n",
    "            for img_path in straight_dir.glob('*.png'):\n",
    "                self.images.append(str(img_path))\n",
    "                self.labels.append(0)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {root_dir}\")\n",
    "        print(f\"Curly: {sum(self.labels)}, Straight: {len(self.labels) - sum(self.labels)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97115414-bdda-4006-9a39-1f8d1b623f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN Model\n",
    "class HairClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HairClassifierCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layer: 32 filters, kernel 3x3, padding=0, stride=1\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, \n",
    "                               kernel_size=(3, 3), padding=0, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Max pooling layer: 2x2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        # Calculate flattened size after conv and pooling\n",
    "        # Input: (3, 200, 200)\n",
    "        # After Conv2d: (32, 198, 198) because (200 - 3 + 0)/1 + 1 = 198\n",
    "        # After MaxPool2d: (32, 99, 99) because 198/2 = 99\n",
    "        self.flatten_size = 32 * 99 * 99\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Output layer (no activation - BCEWithLogitsLoss includes sigmoid)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv + ReLU + MaxPool\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631ef9b6-9438-4fba-88b7-13df9d61dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms with ImageNet normalization\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )  # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e35dd9-7f64-4270-a17a-1156935e5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms with augmentation for second training phase\n",
    "train_transforms_augmented = transforms.Compose([\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )  # ImageNet normalization\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )  # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b22963-af54-409b-9849-47436ae3fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 759 images from data/train\n",
      "Curly: 394, Straight: 365\n",
      "Loaded 193 images from data/test\n",
      "Curly: 100, Straight: 93\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_dataset = HairDataset('data/train', transform=train_transforms)\n",
    "test_dataset = HairDataset('data/test', transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01cdffa6-7343-4c0d-9198-80937df5ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with batch_size=20\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80149ce6-8653-42b8-a47c-b27eefd5697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d49dfe-9e4b-4b60-80a6-5242212af92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HairClassifierCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=313632, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = HairClassifierCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9fb639f-576b-4fe1-b26b-db3807b9ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 20,073,473\n",
      "Trainable parameters: 20,073,473\n"
     ]
    }
   ],
   "source": [
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3a89445-ffb8-4e9a-8ed0-7017ae8f044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter breakdown:\n",
      "conv1.weight: 864 parameters\n",
      "conv1.bias: 32 parameters\n",
      "fc1.weight: 20,072,448 parameters\n",
      "fc1.bias: 64 parameters\n",
      "fc2.weight: 64 parameters\n",
      "fc2.bias: 1 parameters\n"
     ]
    }
   ],
   "source": [
    "# Detailed parameter breakdown\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.numel():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0236047-5978-4858-946c-ec4eb4d1aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits for binary classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe1c2f3-89c1-4fcc-98be-99a6880fc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with history tracking\n",
    "def train_model_with_history(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
    "    history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)  # Ensure labels are float and have shape (batch_size, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = correct_train / total_train\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                labels = labels.float().unsqueeze(1)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(test_dataset)\n",
    "        val_epoch_acc = correct_val / total_val\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "              f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "506b6114-da6f-40bc-a710-c4bd5f42023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch 1/10, Loss: 0.6398, Acc: 0.6324, Val Loss: 0.7477, Val Acc: 0.5751\n",
      "Epoch 2/10, Loss: 0.5635, Acc: 0.7049, Val Loss: 0.6436, Val Acc: 0.6425\n",
      "Epoch 3/10, Loss: 0.5038, Acc: 0.7497, Val Loss: 0.6201, Val Acc: 0.6528\n",
      "Epoch 4/10, Loss: 0.4185, Acc: 0.8076, Val Loss: 0.5979, Val Acc: 0.7098\n",
      "Epoch 5/10, Loss: 0.4140, Acc: 0.8103, Val Loss: 0.7306, Val Acc: 0.6269\n",
      "Epoch 6/10, Loss: 0.3764, Acc: 0.8063, Val Loss: 0.7482, Val Acc: 0.6218\n",
      "Epoch 7/10, Loss: 0.2749, Acc: 0.8893, Val Loss: 0.6358, Val Acc: 0.7306\n",
      "Epoch 8/10, Loss: 0.2098, Acc: 0.9144, Val Loss: 0.7522, Val Acc: 0.7150\n",
      "Epoch 9/10, Loss: 0.1741, Acc: 0.9354, Val Loss: 0.7580, Val Acc: 0.6891\n",
      "Epoch 10/10, Loss: 0.1460, Acc: 0.9486, Val Loss: 0.9511, Val Acc: 0.6632\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "history = train_model_with_history(model, train_loader, test_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "525f822e-d00e-4956-949d-7c5ee0e80db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANSWERS TO HOMEWORK QUESTIONS (First 10 epochs)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for Questions 3 and 4\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWERS TO HOMEWORK QUESTIONS (First 10 epochs)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f11c98d-8dbf-4c96-a348-3acd16d8ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 3 - Median of training accuracy: 0.8090\n",
      "Training accuracies: ['0.6324', '0.7049', '0.7497', '0.8076', '0.8103', '0.8063', '0.8893', '0.9144', '0.9354', '0.9486']\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Median of training accuracy\n",
    "median_train_acc = np.median(history['acc'])\n",
    "print(f\"\\nQuestion 3 - Median of training accuracy: {median_train_acc:.4f}\")\n",
    "print(f\"Training accuracies: {[f'{acc:.4f}' for acc in history['acc']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "821fc27a-3921-4796-ae44-a77d57fba495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 4 - Standard deviation of training loss: 0.1599\n",
      "Training losses: ['0.6398', '0.5635', '0.5038', '0.4185', '0.4140', '0.3764', '0.2749', '0.2098', '0.1741', '0.1460']\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Standard deviation of training loss\n",
    "std_train_loss = np.std(history['loss'])\n",
    "print(f\"\\nQuestion 4 - Standard deviation of training loss: {std_train_loss:.4f}\")\n",
    "print(f\"Training losses: {[f'{loss:.4f}' for loss in history['loss']]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "370651aa-6353-4821-9a44-fd1c104f2167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONTINUING TRAINING WITH DATA AUGMENTATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONTINUE TRAINING WITH DATA AUGMENTATION (Questions 5 and 6)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONTINUING TRAINING WITH DATA AUGMENTATION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de158dc0-e51e-4040-a380-bc2231e0e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 759 images from data/train\n",
      "Curly: 394, Straight: 365\n"
     ]
    }
   ],
   "source": [
    "# Create new training dataset with augmentation\n",
    "train_dataset_augmented = HairDataset('data/train', transform=train_transforms_augmented)\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7821b477-2422-490e-bcb5-5f781ae9d1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuing training with augmented data for 10 more epochs...\n",
      "Epoch 1/10, Loss: 0.6718, Acc: 0.6034, Val Loss: 0.7322, Val Acc: 0.6632\n",
      "Epoch 2/10, Loss: 0.5742, Acc: 0.6825, Val Loss: 0.7526, Val Acc: 0.6684\n",
      "Epoch 3/10, Loss: 0.5535, Acc: 0.6877, Val Loss: 0.7080, Val Acc: 0.6425\n",
      "Epoch 4/10, Loss: 0.5327, Acc: 0.7141, Val Loss: 0.6229, Val Acc: 0.7150\n",
      "Epoch 5/10, Loss: 0.5155, Acc: 0.7339, Val Loss: 0.6375, Val Acc: 0.6891\n",
      "Epoch 6/10, Loss: 0.4970, Acc: 0.7510, Val Loss: 0.6838, Val Acc: 0.6788\n",
      "Epoch 7/10, Loss: 0.4587, Acc: 0.7839, Val Loss: 0.6653, Val Acc: 0.6684\n",
      "Epoch 8/10, Loss: 0.4769, Acc: 0.7655, Val Loss: 0.5778, Val Acc: 0.7306\n",
      "Epoch 9/10, Loss: 0.4639, Acc: 0.7615, Val Loss: 0.7227, Val Acc: 0.6839\n",
      "Epoch 10/10, Loss: 0.4728, Acc: 0.7589, Val Loss: 0.5665, Val Acc: 0.7098\n"
     ]
    }
   ],
   "source": [
    "# Continue training for 10 more epochs (DO NOT recreate the model)\n",
    "print(\"\\nContinuing training with augmented data for 10 more epochs...\")\n",
    "history_augmented = train_model_with_history(model, train_loader_augmented, test_loader, \n",
    "                                             criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40b1f1f4-2cda-4bf5-97ce-fe814afafcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANSWERS TO HOMEWORK QUESTIONS (Augmented training)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for Questions 5 and 6\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANSWERS TO HOMEWORK QUESTIONS (Augmented training)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a00ee49-b9ab-4bf4-b0a8-6431436bdb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 5 - Mean of test loss (all 10 epochs): 0.6669\n",
      "Test losses: ['0.7322', '0.7526', '0.7080', '0.6229', '0.6375', '0.6838', '0.6653', '0.5778', '0.7227', '0.5665']\n"
     ]
    }
   ],
   "source": [
    "# Question 5: Mean of test loss for all epochs\n",
    "mean_test_loss = np.mean(history_augmented['val_loss'])\n",
    "print(f\"\\nQuestion 5 - Mean of test loss (all 10 epochs): {mean_test_loss:.4f}\")\n",
    "print(f\"Test losses: {[f'{loss:.4f}' for loss in history_augmented['val_loss']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13af7caa-472e-47d8-8842-1cdb755aa9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 6 - Average of test accuracy (epochs 6-10): 0.6943\n",
      "Test accuracies (last 5 epochs): ['0.6788', '0.6684', '0.7306', '0.6839', '0.7098']\n",
      "All test accuracies: ['0.6632', '0.6684', '0.6425', '0.7150', '0.6891', '0.6788', '0.6684', '0.7306', '0.6839', '0.7098']\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Average of test accuracy for last 5 epochs (epochs 6-10)\n",
    "avg_test_acc_last5 = np.mean(history_augmented['val_acc'][5:10])\n",
    "print(f\"\\nQuestion 6 - Average of test accuracy (epochs 6-10): {avg_test_acc_last5:.4f}\")\n",
    "print(f\"Test accuracies (last 5 epochs): {[f'{acc:.4f}' for acc in history_augmented['val_acc'][5:10]]}\")\n",
    "print(f\"All test accuracies: {[f'{acc:.4f}' for acc in history_augmented['val_acc']]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dcb39be-54db-4f11-84b8-8745240be083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final evaluation on test set...\n",
      "Final Test Accuracy: 70.98%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on test set\n",
    "print(\"\\nFinal evaluation on test set...\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "final_accuracy = 100 * correct / total\n",
    "print(f'Final Test Accuracy: {final_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3336f3f1-45f5-4a1c-96b3-73d31d0bed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to 'hair_classifier_model.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the model (optional)\n",
    "torch.save(model.state_dict(), 'hair_classifier_model.pth')\n",
    "print(\"\\nModel saved to 'hair_classifier_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd8fb9-e907-4fd5-b8ef-c4f6482322a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
